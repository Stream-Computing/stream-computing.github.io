<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-推理软件使用手册/STCRP使用指南/STC_LLM使用指南" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.1">
<title data-rh="true">STC_LLM使用指南 | 希姆计算文档中心</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://your-docusaurus-site.example.com/docs/推理软件使用手册/STCRP使用指南/STC_LLM使用指南"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="STC_LLM使用指南 | 希姆计算文档中心"><meta data-rh="true" name="description" content="STC_LLM概述"><meta data-rh="true" property="og:description" content="STC_LLM概述"><link data-rh="true" rel="canonical" href="https://your-docusaurus-site.example.com/docs/推理软件使用手册/STCRP使用指南/STC_LLM使用指南"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/推理软件使用手册/STCRP使用指南/STC_LLM使用指南" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/推理软件使用手册/STCRP使用指南/STC_LLM使用指南" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"推理软件使用手册","item":"https://your-docusaurus-site.example.com/docs/category/推理软件使用手册"},{"@type":"ListItem","position":2,"name":"STCRP使用指南","item":"https://your-docusaurus-site.example.com/docs/category/stcrp使用指南"},{"@type":"ListItem","position":3,"name":"STC_LLM使用指南","item":"https://your-docusaurus-site.example.com/docs/推理软件使用手册/STCRP使用指南/STC_LLM使用指南"}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="希姆计算文档中心 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="希姆计算文档中心 Atom Feed"><link rel="stylesheet" href="/assets/css/styles.59b7a052.css">
<script src="/assets/js/runtime~main.128f8459.js" defer="defer"></script>
<script src="/assets/js/main.1dce0f42.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/stc_logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/stc_logo.svg" alt="" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/stc_logo.svg" alt="" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">STC Docs Hub</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/希姆计算术语表">AI加速卡</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/希姆计算术语表">AI一体机</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/希姆计算术语表">智算云平台</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/blog">Blog</a><a href="https://github.com/Stream-Computing/stream-computing.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item green"><a class="menu__link" href="/docs/希姆计算术语表"><span title="希姆计算术语表" class="linkLabel_WmDU">希姆计算术语表</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed red"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/category/硬件产品手册"><span title="硬件产品手册" class="categoryLinkLabel_W154">硬件产品手册</span></a><button aria-label="Expand sidebar category &#x27;硬件产品手册&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed red"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/category/固件产品手册"><span title="固件产品手册" class="categoryLinkLabel_W154">固件产品手册</span></a><button aria-label="Expand sidebar category &#x27;固件产品手册&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item red"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/docs/category/推理软件使用手册"><span title="推理软件使用手册" class="categoryLinkLabel_W154">推理软件使用手册</span></a><button aria-label="Collapse sidebar category &#x27;推理软件使用手册&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item green"><a class="menu__link" tabindex="0" href="/docs/推理软件使用手册/STCRP Release Notes"><span title="STCRP Release Notes" class="linkLabel_WmDU">STCRP Release Notes</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item green"><a class="menu__link" tabindex="0" href="/docs/推理软件使用手册/STCRP产品简介"><span title="STCRP产品简介" class="linkLabel_WmDU">STCRP产品简介</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item green"><a class="menu__link" tabindex="0" href="/docs/推理软件使用手册/STCRP安装指南"><span title="STCRP安装指南" class="linkLabel_WmDU">STCRP安装指南</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item red"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/category/stcrp使用指南"><span title="STCRP使用指南" class="categoryLinkLabel_W154">STCRP使用指南</span></a><button aria-label="Collapse sidebar category &#x27;STCRP使用指南&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item green"><a class="menu__link" tabindex="0" href="/docs/推理软件使用手册/STCRP使用指南/HPE使用指南"><span title="HPE使用指南" class="linkLabel_WmDU">HPE使用指南</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item green"><a class="menu__link" tabindex="0" href="/docs/推理软件使用手册/STCRP使用指南/MLTC使用指南"><span title="MLTC使用指南" class="linkLabel_WmDU">MLTC使用指南</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item green"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/推理软件使用手册/STCRP使用指南/STC_LLM使用指南"><span title="STC_LLM使用指南" class="linkLabel_WmDU">STC_LLM使用指南</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed red"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/category/算力服务解决方案"><span title="算力服务解决方案" class="categoryLinkLabel_W154">算力服务解决方案</span></a><button aria-label="Expand sidebar category &#x27;算力服务解决方案&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed red"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/category/开发者资源"><span title="开发者资源" class="categoryLinkLabel_W154">开发者资源</span></a><button aria-label="Expand sidebar category &#x27;开发者资源&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/category/推理软件使用手册"><span>推理软件使用手册</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/category/stcrp使用指南"><span>STCRP使用指南</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">STC_LLM使用指南</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>STC_LLM使用指南</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="stc_llm概述">STC_LLM概述<a href="#stc_llm概述" class="hash-link" aria-label="Direct link to STC_LLM概述" title="Direct link to STC_LLM概述" translate="no">​</a></h2>
<p>本文档将演示如何基于希姆计算软硬件部署LLM，提供推理服务、处理推理任务并监控推理过程中的指标。</p>
<p>主流的开源LLM往往基于HuggingFace等格式，权重保存为bin或者safetensors，为更高效地完成LLM推理任务，希姆计算面向LLM自研了推理框架STC_LLM。STC_LLM在AI编译器、手写算子的基础上对LLM的推理过程进行了封装，您可以方便地导入、替换LLM并完成推理任务，简化了LLM的部署和使用过程。</p>
<p>STC_LLM面向LLM支持以下特性：</p>
<ul>
<li>
<p>编译阶段：支持权重转换、量化预处理、Tensor并行处理超大规模的权重等。</p>
</li>
<li>
<p>服务阶段：支持动态batch管理优化端到端吞吐。</p>
</li>
<li>
<p>推理阶段：支持kernel级别的优化、KV Cache优化等。</p>
</li>
</ul>
<p>基于STC_LLM的典型推理流程如下图所示：</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-13-d473b1f29ea70b002baf9daafdf3d69e.png" width="1280" height="492" class="img_ev3q"></p>
<p>STC_LLM_DNN是大模型通过手写算子等特殊处理编译后部署LLM的框架。STC_LLM_MLTC是大模型通过MLTC编译器编译后部署LLM的框架。目前，采用STC_LLM_DNN框架进行编译部署的大模型，在推理环节展现出更优的性能表现。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="前提条件">前提条件<a href="#前提条件" class="hash-link" aria-label="Direct link to 前提条件" title="Direct link to 前提条件" translate="no">​</a></h2>
<ul>
<li>
<p>确保服务器的内存满足模型的要求，大模型的HuggingFace项目文件都很大，在转换权重时还会创建副本，因此需要配备尽量大的内存。</p>
</li>
<li>
<p>确保能访问HuggingFace等渠道，且可以下载带有LFS标记的文件。</p>
</li>
<li>
<p>准备LLM推理环境，安装HPE、Python V3.10、HPE Python、STC_LLM。</p>
<ul>
<li>
<p>部署手写大模型时，确保已安装STC_LLM_DNN。</p>
</li>
<li>
<p>通过MLTC编译器编译和部署大模型时，确保已安装好STC_LLM_MLTC和MLTC。</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>说明：各项软件的具体操作，请参见<em>STCRP安装指南</em>。</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="提供推理服务">提供推理服务<a href="#提供推理服务" class="hash-link" aria-label="Direct link to 提供推理服务" title="Direct link to 提供推理服务" translate="no">​</a></h2>
<p>STC_LLM中封装了OpenAI风格的接口，服务端可以提供推理服务并处理推理请求，客户端通过Python脚本、curl命令等方式和LLM交互。我们支持手写大模型和通过MLTC编译器编译大模型，以Qwen2-7B-Instruct为例演示STC_LLM提供推理服务步骤。</p>
<blockquote>
<p>说明：目前支持的LLM列表，请参见<em>模型支持说明</em>。</p>
</blockquote>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="服务端启动推理服务">服务端启动推理服务<a href="#服务端启动推理服务" class="hash-link" aria-label="Direct link to 服务端启动推理服务" title="Direct link to 服务端启动推理服务" translate="no">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="准备模型">准备模型<a href="#准备模型" class="hash-link" aria-label="Direct link to 准备模型" title="Direct link to 准备模型" translate="no">​</a></h4>
<ol>
<li>
<p>从HuggingFace等渠道获取<a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct" target="_blank" rel="noopener noreferrer">Qwen2-7B-Instruct</a>文件，包括模型文件、权重文件、分词器文件、词表等，并复制到目标服务器。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-7903d9c94553bbd81d24818d822c65fa.png" width="1280" height="694" class="img_ev3q"></p>
</li>
<li>
<p>（可选）使用SNC量化工具生成量化系数。详细步骤请参见<em>生成量化模型</em>章节。</p>
</li>
<li>
<p>（可选）在本地验证大模型的可行性，支持本地验证手写大模型和通过MLTC编译器编译大模型，详细步骤请参见<em>本地验证模型</em>章节。</p>
</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="启动推理服务">启动推理服务<a href="#启动推理服务" class="hash-link" aria-label="Direct link to 启动推理服务" title="Direct link to 启动推理服务" translate="no">​</a></h4>
<p>我们支持手写大模型和MLTC编译大模型，在服务端启动推理服务时可以指定编译部署方式。</p>
<ul>
<li>
<p>手写大模型示例：</p>
<ul>
<li>
<p>手动转换权重并启动推理服务。</p>
<ul>
<li>
<p>手动转换权重。</p>
<div class="language-Bash language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ python -m stc_llm_dnn.tools.convert_weight \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-m Qwen/Qwen2-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p /model/Qwen2-7b-Instruct/Qwen2-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-o /model/Qwen2-7b-Instruct/Qwen2-7B-Instruct_2npu_dataset \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-n 2</span><br></span></code></pre></div></div>
<p>配置选项说明：</p>
<table><thead><tr><th><strong>参数选项</strong></th><th><strong>描述</strong></th><th><strong>是否必选</strong></th></tr></thead><tbody><tr><td>-m</td><td>模型在HuggingFace上的ID。</td><td>是</td></tr><tr><td>-p</td><td>原始权重所在的本地路径。</td><td>是</td></tr><tr><td>-o</td><td>转换后的NPU权重的导出路径。</td><td>是</td></tr><tr><td>-n</td><td>指定转换后的NPU权重适配的目标NPU数量，即原始权重将被拆分的份数。为特定数量NPU（如4NPU）转换的权重，无法直接用于不同数量（如2NPU）的 NPU。</td><td>是</td></tr></tbody></table>
</li>
<li>
<p>启动推理服务。</p>
<div class="language-Bash language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ python -m stc_llm.entrypoints.openai.api_server \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--model_name Qwen/Qwen2-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--tok_dir /model/qwen2-7b-instruct/Qwen2-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--weight_dir /model/qwen2-7b-instruct/Qwen2-7B-Instruct/2npu/ \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max_tasks 256 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--custom_parameters \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--temperature 0.5 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--top_p 0.2 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--top_k 1 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max_output_len 4096 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--port 8080 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--device 0 1</span><br></span></code></pre></div></div>
</li>
</ul>
</li>
<li>
<p>自动转换权重并启动推理服务，使用<code>--original_weight_dir</code>原始权重。</p>
<div class="language-Bash language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ python -m stc_llm.entrypoints.openai.api_server \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--model_name Qwen/Qwen2-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--tok_dir /model/qwen2-7b-instruct/Qwen2-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--original_weight_dir=/model/qwen2-7b-instruct/Qwen2-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max_tasks 256 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--custom_parameters \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--temperature 0.5 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--top_p 0.2 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--top_k 1 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max_output_len 4096 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--port 8080 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--device 0 1</span><br></span></code></pre></div></div>
</li>
</ul>
</li>
<li>
<p>MLTC编译大模型并启动推理服务：</p>
<div class="language-Bash language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ python -m stc_llm.entrypoints.openai.api_server \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--model_name Qwen/Qwen2-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--tok_dir /model/qwen2-7b-instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--mltc_weight_dir /model/qwen2-7b-instruct-weight/qwen2-7b-32heads-mask-28layers \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--devices 0 1 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--compiler mltc</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 忽略部分回显</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">INFO:     Application startup complete.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">INFO:     Uvicorn running on http://0.0.0.0:18000 (Press CTRL+C to quit)</span><br></span></code></pre></div></div>
</li>
</ul>
<p>大模型也可使用投机采样方式提高性能。投机采样是一种高效的大模型推理优化策略，通过让同一系列的小模型预先生成多个token，再有大模型批量验证的方式。它巧妙地融合了小模型快速生成token的优势以及大模型精准评估token的能力，从而在提升自回归语言模型推理速度的同时，确保了生成结果的高质量。具体示例可参见<em>投机采样示例</em>章节。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="配置项说明">配置项说明<a href="#配置项说明" class="hash-link" aria-label="Direct link to 配置项说明" title="Direct link to 配置项说明" translate="no">​</a></h4>
<table><thead><tr><th><strong>配置项</strong></th><th><strong>是否必填</strong></th><th><strong>数据类型</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>model_name</td><td>是</td><td>str</td><td>待部署模型的名称，建议与HuggingFace等渠道中的名称保持一致。</td></tr><tr><td>devices</td><td>是</td><td>list</td><td>模型推理时占用NPU设备的ID列表，STC_LLM会根据NPU设备的数量将原始权重自动切分为匹配的份数。</td></tr><tr><td>tok_dir</td><td>是</td><td>str</td><td>加载模型的分词器（tokenizer）文件的路径。</td></tr><tr><td>compiler</td><td>是</td><td>str</td><td>mltc：采用MLTC编译器编译。stc_llm_dnn：默认值，采用手写方式编译。</td></tr><tr><td>embed_on_host</td><td>否</td><td>bool</td><td>- True：启用host侧word_embedding计算。<br>- False：默认值。禁用host侧word_embedding计算。</td></tr><tr><td>output_on_host</td><td>否</td><td>bool</td><td>- True：启用host侧logits计算。<br>- False：默认值。禁用host侧logits计算。</td></tr><tr><td>original_weight_dir</td><td>否</td><td>str</td><td>加载模型的原始权重文件的路径。如果您没有在目标服务器上转换过权重，则至少需要从原始权重转换一次，这时<code>original_weight_dir</code>为必填项。</td></tr><tr><td>weight_dir</td><td>否</td><td>str</td><td>从原始权重转换后可以在NPU设备上使用的权重的路径。<br>- 如果<code>weight_dir</code>指向的路径存在权重文件，则使用这些权重。<br>- 如果<code>weight_dir</code>指向的路径不存在权重文件，则STC_LLM尝试从<code>original_weight_dir</code>获取原始权重进行转换，并将转换后的权重放到<code>weight_dir</code>指向的路径。</td></tr><tr><td>mltc_weight_dir</td><td>是<br>说明：当采用MLTC编译方式时为必填。</td><td>str</td><td>采用MLTC编译方式时的权重路径。</td></tr><tr><td>mltc_ht_file</td><td>否</td><td>str</td><td>采用MLTC编译方式时，vmfb文件存放的路径。</td></tr><tr><td>engine_dir</td><td>否</td><td>str</td><td>临时路径，存放生成的模型代码（cpp）文件。STC_LLM会基于模型代码文件生成NPU设备上的可执行文件，您可以查看模型代码文件了解代码逻辑或者进行模型调试。默认值为<code>.cache/stc_llm</code>。</td></tr><tr><td>slot_number_per_segment</td><td>否</td><td>int</td><td>每个segment占用的slot的数量，对应一组token所需的KV Cache空间。默认值为256。</td></tr><tr><td>max_tasks</td><td>否</td><td>int</td><td>支持同时执行的最大task数量，每个task分配一个stream完成一轮对话。默认值为256。</td></tr><tr><td>custom_parameters</td><td>否</td><td>bool</td><td>- True：允许task自定义temperature、top_p、top_k、logprobs、top_logprobs。<br>- False：默认值。禁止task自定义temperature、top_p、top_k、logprobs、top_logprobs。</td></tr><tr><td>temperature</td><td>否</td><td>float</td><td>超参数之一，控制生成输出的随机性，值越低保证更多的确定性，值越高引入更多的随机性。需要将custom_parameters配置为True才可以修改，默认值为0.5，数值范围为[0.0, 2.0]。</td></tr><tr><td>top_p</td><td>否</td><td>float</td><td>超参数之一，在生成输出抽样时排除累积概率低于该值的token。需要将custom_parameters配置为True才可以修改，默认值为0.2，数值范围为(0.0, 1.0]。</td></tr><tr><td>top_k</td><td>否</td><td>int</td><td>超参数之一，在生成输出抽样时只在概率top k个token中进行。需要将custom_parameters配置为True才可以修改，默认值为20，数值范围为[1, 词表长度]。</td></tr><tr><td>dma_preload</td><td>否</td><td>bool</td><td>- True：默认值。启用DMA预取，通过拷贝权重、矩阵计算等并行操作加速推理。<br>- False：禁用DMA预取。</td></tr><tr><td>render_cpp</td><td>否</td><td>bool</td><td>- True：默认值。部署模型时重新生成并覆盖当前的模型代码（cpp）文件。<br>- False：部署模型时使用当前的模型代码（cpp）文件，不重新生成。</td></tr><tr><td>quant_type</td><td>否</td><td>str</td><td>指定是否通过量化节省内存空间。<br>- fp16：默认值，不启用压缩和量化。<br>- w8a8：启用量化。<br>- w8a16：启用压缩。</td></tr><tr><td>compress_factor</td><td>否</td><td>int</td><td>压缩倍数，仅在<code>quant_type</code>为<code>w8a16</code>时生效，支持的压缩倍数包括128、64、32、16、8、1、-1，压缩倍数越高，能节省的内存空间越多。压缩倍数为-1时对应per-channel方式。</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="客户端发起推理请求">客户端发起推理请求<a href="#客户端发起推理请求" class="hash-link" aria-label="Direct link to 客户端发起推理请求" title="Direct link to 客户端发起推理请求" translate="no">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="发起推理请求">发起推理请求<a href="#发起推理请求" class="hash-link" aria-label="Direct link to 发起推理请求" title="Direct link to 发起推理请求" translate="no">​</a></h4>
<p>在客户端调用completions或者chat_completions接口发起推理请求。</p>
<ul>
<li>
<p>补全文本示例，向LLM提供起始文本，LLM自动将起始文本补全为一段话、文章等形式。</p>
<ul>
<li>
<p>curl方式：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># 调用completions接口示例</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ curl http://172.16.xx.xxx:8000/v1/completions \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -H &quot;Content-Type: application/json&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -d &#x27;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;model&quot;: &quot;Qwen/Qwen2-7B-Instruct&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;prompt&quot;: &quot;你是一个写作小助手，请帮忙写一篇描述江南春天长度不少于1024个token的小作文，要求其中必须涉及到描述朦胧的烟雨，蜿蜒的石板小路等景物，同时既要描写出阳光明媚、风和日丽的场景，也要描述出烟雨绵绵、云气氤氲的场景。然后文章中需要包含排比、比喻、对偶、拟人等修辞手法，必要时可以引用一些符合文章主题语境的古诗词中的句子以增加文采。&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;max_tokens&quot;: 256,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{&quot;model&quot;:&quot;Qwen/Qwen2-7B-Instruct&quot;,&quot;object&quot;:&quot;text_completion&quot;,&quot;created&quot;:1736219436,&quot;choices&quot;:[{&quot;index&quot;:0,&quot;text&quot;:&quot;江南的春天，如同一位羞涩的少女，轻启朱唇，吐露着温柔的气息。她以一种难以言喻的美，缓缓铺展开一幅幅细腻的画卷，让人心醉神迷。在这片土地上，春天的长度仿佛被时间的笔触拉长，每一刻都充满了诗意与生机。\n\n烟雨是江南春天的魂魄，它轻柔地拂过每一寸土地，为大地披上了一层薄薄的轻纱。那烟雨，如同细丝般缠绕在古老的石板小路上，蜿蜒曲折，仿佛是大自然的笔触，勾勒出一幅幅水墨画。雨滴落在青石板上，发出清脆的响声，如同古琴的低吟，悠扬而深邃。这烟雨，是江南春天的序曲，它以一种朦胧而神秘的方式，唤醒了沉睡的大地，让万物在湿润中苏醒。\n\n阳光明媚的日子，江南的春天则展现出另一番景象。阳光如同金色的丝线，温柔地洒在大地上，将万物染上了一层金黄的光泽。微风轻拂，带着花香与泥土的芬芳，让人感到心旷&quot;,&quot;finish_reason&quot;:&quot;stop&quot;}]}</span><br></span></code></pre></div></div>
</li>
<li>
<p>Python脚本方式：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># 调用completions接口示例</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ pip3 install openai</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ cat test_qwen2-7b-instruct-completions.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from openai import OpenAI</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">openai_api_key = &quot;EMPTY&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">openai_api_base = &quot;http://172.16.xxx.xxx:8000/v1&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">client = OpenAI(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    api_key=openai_api_key,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    base_url=openai_api_base,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">completion = client.completions.create(model=&quot;Qwen/Qwen2-7B-Instruct&quot;, prompt=&quot;San Francisco is a&quot;, max_tokens=50)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ python3 test_qwen2-7b-instruct-completions.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Completion result: Completion(id=None, choices=[CompletionChoice(finish_reason=&#x27;stop&#x27;, index=0, logprobs=None, text=&#x27;San Francisco is a city located in the northern part of California, United States. It is known for its iconic landmarks such as the Golden Gate Bridge, Alcatraz Island, and the hilly landscape. San Francisco is also famous for its diverse culture&#x27;)], created=1736235685, model=&#x27;Qwen/Qwen2-7B-Instruct&#x27;, object=&#x27;text_completion&#x27;, system_fingerprint=None, usage=None)</span><br></span></code></pre></div></div>
</li>
<li>
<p>发起聊天示例，向LLM提供系统角色信息以及聊天输入，LLM理解输入并按其系统角色回复用户。</p>
<ul>
<li>
<p>curl方式：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># 调用chat_completions接口示例</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ curl http:/localhost:8000/v1/chat/completions \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -H &quot;Content-Type: application/json&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -d &#x27;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;model&quot;: &quot;Qwen/Qwen2-7B-Instruct&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;messages&quot;: [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me a joke.&quot;}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{&quot;model&quot;:&quot;Qwen/Qwen2-7B-Instruct&quot;,&quot;object&quot;:&quot;chat.completion&quot;,&quot;choices&quot;:[{&quot;index&quot;:0,&quot;message&quot;:{&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;Sure, here&#x27;s a classic joke:\n\nWhy did the tomato turn red?\n\nBecause it saw the salad dressing!&quot;,&quot;function_call&quot;:null},&quot;finish_reason&quot;:&quot;stop&quot;,&quot;logprobs&quot;:{&quot;content&quot;:null}}],&quot;created&quot;:1736235856}</span><br></span></code></pre></div></div>
</li>
<li>
<p>Python脚本示例：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># 调用completions接口示例</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ pip3 install openai</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ cat test_qwen2-7b-instruct-chat-completions.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from openai import OpenAI</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">openai_api_key = &quot;EMPTY&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">openai_api_base = &quot;http://172.16.xxx.xxx:8000/v1&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">client = OpenAI(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    api_key=openai_api_key,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    base_url=openai_api_base,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">chat_response = client.chat.completions.create(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model=&quot;Qwen/Qwen2-7B-Instruct&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    messages=[</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me a joke.&quot;},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_tokens=256</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">print(&quot;Chat response:&quot;, chat_response)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ python3 test_qwen2-7b-instruct-chat-completions.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Chat response: ChatCompletion(id=None, choices=[Choice(finish_reason=&#x27;stop&#x27;, index=0, logprobs=ChoiceLogprobs(content=None, refusal=None), message=ChatCompletionMessage(content=&quot;Sure, here&#x27;s a classic joke:\n\nWhy did the tomato turn red?\n\nBecause it saw the salad dressing!&quot;, refusal=None, role=&#x27;assistant&#x27;, audio=None, function_call=None, tool_calls=None))], created=1736236505, model=&#x27;Qwen/Qwen2-7B-Instruct&#x27;, object=&#x27;chat.completion&#x27;, service_tier=None, system_fingerprint=None, usage=None)</span><br></span></code></pre></div></div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="completions参数">completions参数<a href="#completions参数" class="hash-link" aria-label="Direct link to completions参数" title="Direct link to completions参数" translate="no">​</a></h4>
<table><thead><tr><th><strong>参数名</strong></th><th><strong>是否必填</strong></th><th><strong>数据类型</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>model</td><td>是</td><td>str</td><td>待调用模型的名称，一般为启动推理服务时所运行的模型名称。</td></tr><tr><td>prompt</td><td>是</td><td>str</td><td>描述要求模型完成的任务，例如补全文本、回答问题。</td></tr><tr><td>temperature</td><td>否</td><td>float</td><td>超参数之一，控制生成输出的随机性，值越低保证更多的确定性，值越高引入更多的随机性。需要在部署模型时将custom_parameters配置为True才可以修改，数值范围为[0.0, 2.0]。</td></tr><tr><td>top_p</td><td>否</td><td>float</td><td>超参数之一，在生成输出抽样时排除累积概率低于该值的token。需要在部署模型时将custom_parameters配置为True才可以修改，数值范围为(0.0, 1.0]。</td></tr><tr><td>top_k</td><td>否</td><td>int</td><td>超参数之一，在生成输出抽样时只在概率top k个token中进行。需要在部署模型时将custom_parameters配置为True才可以修改，数值范围为[1, 词表长度]。</td></tr><tr><td>max_tokens</td><td>否</td><td>int</td><td>最长返回的token数量。数值范围为[1, 模型支持上下文长度]。</td></tr><tr><td>logprobs</td><td>否</td><td>bool</td><td>需要在部署模型时将custom_parameters配置为True才可以修改。<br>- true：启用返回生成输出token的对数概率。<br>- false：默认值，禁用返回生成输出token的对数概率。</td></tr><tr><td>top_logprobs</td><td>否</td><td>int</td><td>指定在每个生成输出token时一并返回最有可能的token数量，每个token都带有对数概率。需要将logprobs开关设置为true方可生效，数值范围为[0, 5]。</td></tr><tr><td>stream</td><td>否</td><td>bool</td><td>- true：启用流式传输，当模型生成一定数量的token后，立即将token传输给客户端，而不是等所有token生成完毕后，从而减少用户的等待时间。<br>- false：默认值，禁用流式传输，等所有token生成完毕后，再将所有token传输给客户端。</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="chat_completions参数">chat_completions参数<a href="#chat_completions参数" class="hash-link" aria-label="Direct link to chat_completions参数" title="Direct link to chat_completions参数" translate="no">​</a></h4>
<table><thead><tr><th><strong>参数名</strong></th><th><strong>是否必填</strong></th><th><strong>数据类型</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>model</td><td>是</td><td>str</td><td>待调用模型的名称，一般为启动推理服务时所运行的模型名称。</td></tr><tr><td>messages</td><td>是</td><td>dict</td><td>传给模型的消息对象，您可以通过字段组合控制模型行为，获得预期的输出。<br>- role字段：用于设置角色，例如system（系统角色）、user（用户角色）。<br>- content字段：用于描述要求，例如通过system描述助手扮演的系统角色，通过user描述用户要求助手完成创作文章、回答问题等任务。</td></tr><tr><td>temperature</td><td>否</td><td>float</td><td>超参数之一，控制生成输出的随机性，值越低保证更多的确定性，值越高引入更多的随机性。需要在部署模型时将custom_parameters配置为True才可以修改，数值范围为[0.0, 2.0]。</td></tr><tr><td>top_p</td><td>否</td><td>float</td><td>超参数之一，在生成输出抽样时排除累积概率低于该值的token。需要在部署模型时将custom_parameters配置为True才可以修改，数值范围为(0.0, 1.0]。</td></tr><tr><td>top_k</td><td>否</td><td>int</td><td>超参数之一，在生成输出抽样时只在概率top k个token中进行。需要在部署模型时将custom_parameters配置为True才可以修改，数值范围为[1, 词表长度]。</td></tr><tr><td>max_tokens</td><td>否</td><td>int</td><td>最长返回的token数量。数值范围为[1, 模型支持上下文长度]。</td></tr><tr><td>logprobs</td><td>否</td><td>bool</td><td>需要在部署模型时将custom_parameters配置为True才可以修改。<br>- true：启用返回生成输出token的对数概率。<br>- false：默认值，禁用返回生成输出token的对数概率。</td></tr><tr><td>top_logprobs</td><td>否</td><td>int</td><td>指定在每个生成输出token时一并返回最有可能的token数量，每个token都带有对数概率。需要将logprobs开关设置为true方可生效，数值范围为[0, 5]。</td></tr><tr><td>stream</td><td>否</td><td>bool</td><td>- true：启用流式传输，当模型生成一定数量的token后，立即将token传输给客户端，而不是等所有token生成完毕后，从而减少用户的等待时间。<br>- false：默认值，禁用流式传输，等所有token生成完毕后，再将所有token传输给客户端。</td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="监控推理指标">监控推理指标<a href="#监控推理指标" class="hash-link" aria-label="Direct link to 监控推理指标" title="Direct link to 监控推理指标" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="前提条件-1">前提条件<a href="#前提条件-1" class="hash-link" aria-label="Direct link to 前提条件" title="Direct link to 前提条件" translate="no">​</a></h3>
<ul>
<li>已在服务端启动推理服务。</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="指标类型">指标类型<a href="#指标类型" class="hash-link" aria-label="Direct link to 指标类型" title="Direct link to 指标类型" translate="no">​</a></h3>
<p>STC_LLM基于Prometheus、Grafana生态实现了监控功能，遵循Prometheus expression语法添加需要监控的推理指标即可。支持的推理指标包括：</p>
<table><thead><tr><th><strong>推理指标</strong></th><th><strong>推理指标含义</strong></th><th><strong>Prometheus expression示例</strong></th></tr></thead><tbody><tr><td>prompt处理吞吐</td><td>每秒处理输入prompt token的吞吐量</td><td><code>stc_llm:avg_prompt_throughput_toks_per_s{model_name=&quot;THUDM/chatglm3-6b&quot;}</code></td></tr><tr><td>generation处理吞吐</td><td>每秒输出token的吞吐量</td><td><code>stc_llm:avg_generation_throughput_toks_per_s{model_name=&quot;THUDM/chatglm3-6b&quot;}</code></td></tr><tr><td>推理次数</td><td>启动推理服务后完成推理的次数</td><td><code>stc_llm:time_to_inference_count{model_name=&quot;THUDM/chatglm3-6b&quot;}</code></td></tr><tr><td>首token时延</td><td>输出首字token花费的时间</td><td><code>stc_llm:first_token_time{model_name=&quot;THUDM/chatglm3-6b&quot;}</code></td></tr><tr><td>任务推理总时延</td><td>完成单轮输出花费的时间</td><td><code>stc_llm:task_duration{model_name=&quot;THUDM/chatglm3-6b&quot;}</code></td></tr><tr><td>任务decoder时延</td><td>从输出首字token开始，到完成单轮输出所花费的时间</td><td><code>stc_llm:task_decoder_latency{model_name=&quot;THUDM/chatglm3-6b&quot;}</code></td></tr><tr><td>avg generation latency</td><td>输出单位token所花费的时间</td><td><code>stc_llm:avg_generation_latency{model_name=&quot;THUDM/chatglm3-6b&quot;}</code></td></tr><tr><td>running tasks count</td><td>进行中的对话任务数量</td><td><code>stc_llm:num_requests_running{model_name=&quot;THUDM/chatglm3-6b&quot;}</code></td></tr><tr><td>swapped tasks count</td><td>因故停止的对话任务数量</td><td><code>stc_llm:num_requests_swapped{model_name=&quot;THUDM/chatglm3-6b&quot;}</code></td></tr><tr><td>waiting tasks count</td><td>等待中的对话任务数量</td><td><code>stc_llm:num_requests_waiting{model_name=&quot;THUDM/chatglm3-6b&quot;}</code></td></tr><tr><td>prompt tokens total count</td><td>输入prompt token的总数量</td><td><code>stc_llm:prompt_tokens_total{model_name=&quot;THUDM/chatglm3-6b&quot;}</code></td></tr><tr><td>generation tokens total count</td><td>输出token的总数量</td><td><code>stc_llm:generation_tokens_total{model_name=&quot;THUDM/chatglm3-6b&quot;}</code></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="服务端操作">服务端操作<a href="#服务端操作" class="hash-link" aria-label="Direct link to 服务端操作" title="Direct link to 服务端操作" translate="no">​</a></h3>
<p>您需要在服务端安装Prometheus、Grafana并确保相关服务正常运行。</p>
<ol>
<li>
<p>安装Prometheus。请根据服务器情况选择合适的安装方式，例如APT源、离线安装包等，相关说明可以参见<a href="https://github.com/prometheus/prometheus" target="_blank" rel="noopener noreferrer">Prometheus官方开源项目</a>。</p>
</li>
<li>
<p>修改Prometheus配置，默认配置文件为<code>/etc/prometheus/prometheus.yml</code>。</p>
<ul>
<li>
<p>提供Prometheus服务的地址，默认端口号为9090。</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">job_name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;prometheus&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">static_configs</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">targets</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;localhost:9090&quot;</span><span class="token punctuation" style="color:#393A34">]</span><br></span></code></pre></div></div>
</li>
<li>
<p>监控推理服务的地址，在对应job（即通过stc_llm.entrypoints.openai.api_server启动的推理服务）的<code>scrape_configs</code>字段下添加IP地址和端口号即可。</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">job_name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;openai_api&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">scrape_interval</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> 5s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">static_configs</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">targets</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;localhost:18000&quot;</span><span class="token punctuation" style="color:#393A34">]</span><br></span></code></pre></div></div>
</li>
</ul>
</li>
<li>
<p>启动Prometheus服务并确认服务状态。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ sudo systemctl start prometheus</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ sudo systemctl status prometheus</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">prometheus.service - Prometheus</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     Loaded: loaded (/etc/systemd/system/prometheus.service; enabled; vendor preset: enabled)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     Active: active (running) since Mon 2024-08-19 16:24:11 CST; 1h 6min ago</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Main PID: 3112690 (prometheus)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      Tasks: 53 (limit: 629145)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     Memory: 48.4M</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     CGroup: /system.slice/prometheus.service</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">             └─3112690 /usr/local/bin/prometheus --config.file /etc/prometheus/prometheus.yml --storage.tsdb.path /var/lib/prometheus/ --web.console.templates=/etc/prometheus/consoles --web.console.libra&gt;</span><br></span></code></pre></div></div>
</li>
<li>
<p>安装Grafana。请根据服务器情况选择合适的安装方式，例如APT源、离线安装包等，相关说明可以参见<a href="https://github.com/grafana/grafana" target="_blank" rel="noopener noreferrer">Grafana官方开源项目</a>。</p>
</li>
<li>
<p>修改Grafana配置，默认配置文件为<code>/etc/grafana/grafana.ini</code>。</p>
<ul>
<li>
<p>提供Grafana服务的地址，默认端口号为3000。</p>
</li>
<li>
<p>登录Grafana Web端的用户名和密码。</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">http_port = 3000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">admin_user = admin</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">admin_password = 123.com</span><br></span></code></pre></div></div>
</li>
</ul>
</li>
<li>
<p>启动Grafana服务并确认服务状态。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ sudo systemctl start grafana-server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ sudo systemctl status grafana-server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">grafana-server.service - Grafana instance</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     Loaded: loaded (/lib/systemd/system/grafana-server.service; disabled; vendor preset: enabled)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     Active: active (running) since Mon 2024-08-19 10:01:25 CST; 7h ago</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       Docs: http://docs.grafana.org</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Main PID: 2970010 (grafana)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      Tasks: 38 (limit: 629145)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     Memory: 53.8M</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     CGroup: /system.slice/grafana-server.service</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">             └─2970010 /usr/share/grafana/bin/grafana server --config=/etc/grafana/grafana.ini --pidfile=/run/grafana/grafana-server.pid --packaging=deb cfg:default.paths.logs=/var/log/grafana cfg:defaul&gt;</span><br></span></code></pre></div></div>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="web端操作prometheus">Web端操作（Prometheus）<a href="#web端操作prometheus" class="hash-link" aria-label="Direct link to Web端操作（Prometheus）" title="Direct link to Web端操作（Prometheus）" translate="no">​</a></h3>
<p>Prometheus提供了指标收集和告警等监控功能，您可以登录Prometheus的Web页面管理需要监控的推理指标。</p>
<ol>
<li>
<p>访问Prometheus Web页面。如果Prometheus服务使用了默认端口，则Web页面地址为<code>http://{server_ip}:9090</code>。</p>
</li>
<li>
<p>单击<strong>Add Panel</strong>添加指标面板。</p>
</li>
<li>
<p>在输入框中填写Prometheus expression，以监控THUDM/chatglm3-6b模型的generation吞吐量为例，填写<code>stc_llm:avg_generation_throughput_toks_per_s{model_name=&quot;THUDM/chatglm3-6b&quot;}</code>。</p>
</li>
<li>
<p>单击<strong>Execute</strong>完成指标添加。</p>
</li>
<li>
<p>单击<strong>Graph</strong>，调整时间范围至执行了推理任务的时间段，即可查看到对应的推理指标。</p>
</li>
</ol>
<p><img decoding="async" loading="lazy" src="/assets/images/8b3a65e1-89a3-4426-b29a-0baa72f5c556-ce104fb2f80c58f8c194683673cd3f94.png" width="1280" height="598" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="web端操作grafana">Web端操作（Grafana）<a href="#web端操作grafana" class="hash-link" aria-label="Direct link to Web端操作（Grafana）" title="Direct link to Web端操作（Grafana）" translate="no">​</a></h3>
<p>Grafana提供了丰富和美观的可视化功能，使用时将Prometheus添加为数据源并创建Dashboard汇总Panel，即可一站式多维度展示推理指标。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="登录grafana">登录Grafana<a href="#登录grafana" class="hash-link" aria-label="Direct link to 登录Grafana" title="Direct link to 登录Grafana" translate="no">​</a></h4>
<ol>
<li>
<p>访问Grafana Web页面。如果Grafana服务使用了默认端口，则Web页面地址为<code>http://{server_ip}:3000</code>。</p>
</li>
<li>
<p>输入配置的用户名和密码。</p>
</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="添加prometheus数据源">添加Prometheus数据源<a href="#添加prometheus数据源" class="hash-link" aria-label="Direct link to 添加Prometheus数据源" title="Direct link to 添加Prometheus数据源" translate="no">​</a></h4>
<ol>
<li>
<p>在左侧导航栏，单击<strong>Connections</strong> &gt; <strong>Data sources</strong>。</p>
</li>
<li>
<p>单击<strong>Add datasource</strong>。</p>
</li>
<li>
<p>选择<strong>Prometheus</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-1-2d4b4bd70ac388f727f5c0a4c22617e3.png" width="1280" height="513" class="img_ev3q"></p>
</li>
<li>
<p>在Settings页面，完成Name、Prometheus server URL等配置，然后单击<strong>Save &amp; Test。</strong>提示<code>Successfully queried the Prometheus API.</code>，即代表数据源添加成功。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-2-c4a872030cd9a65231abadb517ce0780.png" width="1280" height="543" class="img_ev3q"></p>
</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="创建stc_llm-dashboard">创建STC_LLM Dashboard<a href="#创建stc_llm-dashboard" class="hash-link" aria-label="Direct link to 创建STC_LLM Dashboard" title="Direct link to 创建STC_LLM Dashboard" translate="no">​</a></h4>
<ol>
<li>
<p>在左侧导航栏，单击<strong>Dashboards</strong>。</p>
</li>
<li>
<p>单击<strong>New</strong> &gt; <strong>New dashboard</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-3-c1be2a06c8ec376c0b939c244cc64a8c.png" width="1280" height="280" class="img_ev3q"></p>
</li>
<li>
<p>单击<strong>Add visualization</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-4-fac2c22d70b1848d8411470776583a5b.png" width="901" height="467" class="img_ev3q"></p>
</li>
<li>
<p>选择已添加的Prometheus数据源。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-5-7590355785434bbe6855bb316566ebde.png" width="1180" height="698" class="img_ev3q"></p>
</li>
<li>
<p>添加Panel。切换到Code模式，在输入框中填写Prometheus expression，以监控THUDM/chatglm3-6b模型的推理次数为例，填写<code>stc_llm:time_to_inference_count{model_name=&quot;THUDM/chatglm3-6b&quot;}</code>，单击<strong>Run queries</strong>查看效果，然后单击<strong>Save</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-6-83382ebe46319483353cd7b5c40e8bff.png" width="1280" height="598" class="img_ev3q"></p>
</li>
<li>
<p>按提示填入信息，然后单击<strong>Save</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-7-0cbd9a123bc1b5dd04bb5e69ce54685c.png" width="952" height="402" class="img_ev3q"></p>
</li>
<li>
<p>在Dashboards页面即可看到新添加的STC_LLM Dashboard。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-8-07140134febc592e08077200b8012f86.png" width="1280" height="598" class="img_ev3q"></p>
</li>
<li>
<p>进入STC_LLM Dashboard，即可查看已添加的Panel。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-9-4848b5600a30637554157fce5d3ef79f.png" width="1280" height="598" class="img_ev3q"></p>
</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="编辑dashboard中的panel">编辑Dashboard中的Panel<a href="#编辑dashboard中的panel" class="hash-link" aria-label="Direct link to 编辑Dashboard中的Panel" title="Direct link to 编辑Dashboard中的Panel" translate="no">​</a></h4>
<ol>
<li>
<p>在左侧导航栏，单击<strong>Dashboards</strong>。</p>
</li>
<li>
<p>单击STC_LLM Dashboard。</p>
</li>
<li>
<p>在待修改Panel右上角，单击Menu图标 &gt; <strong>Edit</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-10-672e85ba99b25e9ea28091de50babc3f.png" width="802" height="306" class="img_ev3q"></p>
</li>
<li>
<p>按需修改Panel信息，例如将Title修改为推理次数，然后单击<strong>Save</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-11-190c1ce8e36134a69200454f25960323.png" width="1280" height="598" class="img_ev3q"></p>
</li>
<li>
<p>按需填写信息，然后单击<strong>Save</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-12-f1c982b23e5eed02ba6b8ae9588e53bb.png" width="952" height="304" class="img_ev3q"></p>
</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="为dashboard添加新panel">为Dashboard添加新Panel<a href="#为dashboard添加新panel" class="hash-link" aria-label="Direct link to 为Dashboard添加新Panel" title="Direct link to 为Dashboard添加新Panel" translate="no">​</a></h4>
<ol>
<li>
<p>在左侧导航栏，单击<strong>Dashboards</strong>。</p>
</li>
<li>
<p>单击STC_LLM Dashboard。</p>
</li>
<li>
<p>在Dashboard中单击<strong>Add</strong> &gt; <strong>Visualization</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-14-5b99a4172b514d915afa16adc9499b85.png" width="1280" height="598" class="img_ev3q"></p>
</li>
<li>
<p>添加Panel。切换到Code模式，在输入框中填写Prometheus expression，以监控THUDM/chatglm3-6b模型的prompt处理吞吐和generation处理吞吐为例，分别填写<code>stc_llm:avg_prompt_throughput_toks_per_s{model_name=&quot;THUDM/chatglm3-6b&quot;}</code>和<code>stc_llm:avg_generation_throughput_toks_per_s{model_name=&quot;THUDM/chatglm3-6b&quot;}</code>，单击<strong>Run queries</strong>查看效果，然后单击<strong>Save</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-15-35b134636361ae4412e135f8b338be26.png" width="1280" height="598" class="img_ev3q"></p>
</li>
<li>
<p>按要求填入信息，然后单击<strong>Save</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-16-b8653820cadf6056396c2ef702ccbc51.png" width="944" height="314" class="img_ev3q"></p>
</li>
<li>
<p>进入进入STC_LLM Dashboard，即可查看展示效果。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-17-233a6e669b8fe4569825db22e83d4a9c.png" width="1280" height="598" class="img_ev3q"></p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="生成量化系数">生成量化系数<a href="#生成量化系数" class="hash-link" aria-label="Direct link to 生成量化系数" title="Direct link to 生成量化系数" translate="no">​</a></h2>
<p>SNC量化工具是基于英特尔的Neural Compressor构建，支持PyTorch大模型量化。Neural Compressor是一个开源的Python库，它支持在主流深度学习框架上应用流行的模型压缩技术，包括量化、剪枝（稀疏性）、蒸馏和神经架构搜索。SNC量化工具支持仅使用CPU进行模型量化。</p>
<blockquote>
<p>说明：若有ONNX小模型量化需求，可选择使用SNQ模型量化工具，详情可参见<em>MLTC使用指南</em>。</p>
</blockquote>
<p>以Qwen2-7B-Instruct为例演示执行步骤：</p>
<ol>
<li>
<p>准备好数据集和模型文件。</p>
<ul>
<li>
<p>从HuggingFace等渠道获取<a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct" target="_blank" rel="noopener noreferrer">Qwen2-7B-Instruct</a>的文件，包括模型文件、权重文件、分词器文件、词表等，并复制到目标服务器。</p>
</li>
<li>
<p>将准备好的数据集复制到目标服务器。</p>
</li>
</ul>
</li>
<li>
<p>配置YAML文件。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># 用于拉平激活值与权重的超参数列表</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">alpha_list: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 数据集位置路径</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">data_path: &quot;./ceval-exam/val&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 模型位置路径</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_path : &quot;qwen2-7b-instruct&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">#每个数据集子集取多少个数据进行量化</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">calib_size : 1 </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 运行设备，支持CPU和GPU，取值分别为cpu和cuda。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">device : &quot;cpu&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 层数</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">layers: 28</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># attention中有同一输入的linear层名，可以在model.safetensors.index.json中查看，</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">qkv: [&quot;model.layers.{}.self_attn.q_proj&quot;, &quot;model.layers.{}.self_attn.k_proj&quot;, &quot;model.layers.{}.self_attn.v_proj&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># MLP中有同一输入的linear层名</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">up_gate: [&quot;model.layers.{}.mlp.up_proj&quot;, &quot;model.layers.{}.mlp.gate_proj&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 保存scale路径，这里需要完整的绝对路径，路径中不可使用‘~’</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">scale_path: &quot;/root/snc/qwen2_7b_zwt&quot;</span><br></span></code></pre></div></div>
<p>Qwen2-7B-Instruct模型的层数、qkv和up_gate对应值可以模型的配置文件中查询。</p>
<ul>
<li>
<p>layers对应的是模型层数。</p>
</li>
<li>
<p>qkv对应的是attention中有同一输入的linear层名。这些名称是基于<code>model.safetensors.index.json</code>文件中的节点名称进行转换的。转换过程包括将节点名称中的层编号替换为占位符<code>{}</code>，并省略掉<code>o_proj</code>、<code>down_proj</code>、<code>bias</code>、<code>weight</code>等后缀。例如，如果原始节点名称是<code>model.layers.0.self_attn.k_proj.bias</code>，那么在<code>qkv</code>参数中对应的名称就会是<code>model.layers.{}.self_attn.k_proj</code>。</p>
</li>
<li>
<p>up_gate对应的是MLP中有同一输入的linear层名，但不包括down_proj组件。这个名称是根据<code>model.safetensors.index.json</code>文件中的节点名称转换而来的。转换规则是将节点名称中的层编号替换为占位符<code>{}</code>，并移除名称中的<code>weight</code>部分。例如，如果原始节点名称是<code>model.layers.0.mlp.gate_proj.weight</code>，那么<code>up_gate</code>对应的名称将变为<code>model.layers.{}.mlp.gate_proj</code>。</p>
</li>
</ul>
</li>
<li>
<p>设置环境变量，使用SQEX设置YAML文件的存放目录。路径必须为绝对路径。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ export SQEX=/root/snc/sqex.yaml</span><br></span></code></pre></div></div>
</li>
<li>
<p>启动量化工具脚本。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ run_quant_no_eval</span><br></span></code></pre></div></div>
<p>在设置的scale路径下会生成对应超参数的所有候选的量化缩放系数。文件名由<code>sq</code>+超参数组合，sq是smooth quant的简称。smooth quant是一种拉平激活值来减少精度损失的算法。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ cd /root/snc/qwen2_7b_zwt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ tree</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── sq_0.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_qkv_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── act_scale_qkv_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── sq_0.2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_qkv_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── act_scale_qkv_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── sq_0.3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_qkv_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── act_scale_qkv_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── sq_0.4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_qkv_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── act_scale_qkv_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── sq_0.5</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_qkv_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── act_scale_qkv_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── sq_0.6</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_qkv_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── act_scale_qkv_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── sq_0.7</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_qkv_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── act_scale_qkv_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── sq_0.8</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_mlp_up_gate_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── act_scale_qkv_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── act_scale_qkv_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">└── sq_0.9</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ├── act_scale_mlp_up_gate_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ├── act_scale_mlp_up_gate_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ├── act_scale_qkv_v_w.npy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    └── act_scale_qkv_w.npy</span><br></span></code></pre></div></div>
<p>其中文件名后缀为<code>_v_w</code>的文件中存放着对应超参数的激活层的smooth_quant系数，<code>_w</code>的文件中存放着对应超参数的每层pertensor系数。</p>
</li>
<li>
<p>后续可参考<em>本地验证模型</em>章节中的示例，对同一个模型应用不同的量化系数分析精度，从中选出精度最优的量化缩放系数。将scale目录下的量化系数文件放模型配置中original_weight_dir参数指定的目录下，同时quant_type配置设为<code>w8a8</code>。</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="本地验证模型">本地验证模型<a href="#本地验证模型" class="hash-link" aria-label="Direct link to 本地验证模型" title="Direct link to 本地验证模型" translate="no">​</a></h2>
<p>我们支持手写大模型和MLTC编译大模型，您可先在本地部署验证模型。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="手写大模型">手写大模型<a href="#手写大模型" class="hash-link" aria-label="Direct link to 手写大模型" title="Direct link to 手写大模型" translate="no">​</a></h3>
<p>以chatglm3-6b为例演示部署手写大模型步骤：</p>
<ol>
<li>
<p>设置环境变量。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ export RISCV=/usr/local/hpe</span><br></span></code></pre></div></div>
</li>
<li>
<p>从HuggingFace等渠道获取<a href="https://huggingface.co/THUDM/chatglm3-6b/tree/main" target="_blank" rel="noopener noreferrer">chatglm3-6b</a>的文件，包括模型文件、权重文件、分词器文件、词表等，并复制到目标服务器。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/img_v3_02cq_5186f449-e627-4ad0-a098-8bdf2e93bafg-2b6b8152a3a40f68906d423596ae6508.jpg" width="1280" height="723" class="img_ev3q"></p>
</li>
</ol>
<ol start="3">
<li>
<p>编写Python脚本部署并验证模型。脚本示例中主要包括以下步骤：</p>
<ol>
<li>
<p>准备模型配置，支持的配置项请参见<em>模型配置</em>章节。</p>
</li>
<li>
<p>调用<code>AsyncGeneration</code>接口部署模型。</p>
</li>
<li>
<p>发送messages验证LLM的对话效果。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ cat test_chatglm3-6b.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import asyncio</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from stc_llm_dnn.runtime import AsyncGeneration</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">async def print_stream(gen, model, gen_name, prompt):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    async for task in gen:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        print(&quot;[{}] prompt = {}, gen = {}&quot;.format(gen_name, prompt, model.decode_token_ids(task.gen_tokens)))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">async def test_async_generation():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # 配置信息</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    config = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;model_name&quot;: &quot;THUDM/chatglm3-6b&quot;,           # 模型名称</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;devices&quot;: [0, 1],                           # 使用的 npu id 列表</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;tok_dir&quot;: &quot;./chatglm3-6b&quot;,             # tokenizer 目录</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;original_weight_dir&quot;: &quot;./chatglm3-6b&quot;, # 原始权重路径</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;weight_dir&quot;: &quot;chatglm2-6b-2npu-dataset&quot;,    # 转换后权重路径</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;max_tasks&quot;: 256,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;custom_parameters&quot;: True,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;quant_type&quot;: &quot;fp16&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;compress_factor&quot;: 1,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # 创建 generation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    generation = AsyncGeneration(config)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # 运行测试</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    messages = [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是资深的技术支持专家，可以为用户提供软硬件产品的技术支持，解答用户使用产品时的疑问。&quot;},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好，你是谁啊？&quot;},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    task = await generation.create_task(messages=messages, temperature=0.5, top_p=0.2, top_k=20, max_output_len=256)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    gen = generation.generate_stream(task)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    user_task = asyncio.create_task(print_stream(gen, generation.model, f&quot;gen&quot;, &quot;你好&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    await user_task</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    await generation.shutdown()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">if __name__ == &quot;__main__&quot;:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    asyncio.run(test_async_generation())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ python3 test_chatglm3-6b.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 忽略部分回显</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">---------------------------- Model Info Begin ----------------------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model name: THUDM/chatglm3-6b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">npus: [0, 1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">total slot number: 157053</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">slot number per segment: 256</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">total segment number: 613</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bytes per slot: 14336</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">temporary cpp dir: /home/superadmin/.cache/stc_llm_dnn/THUDM/chatglm3-6b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tokenizer dir: ./chatglm3-6b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">weight dir: chatglm2-6b-2npu-dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">embedding on host: False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">output on host: False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dma preload: True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">max sequence length: 8192</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">compress_factor: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">quant_type: fp16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">---------------------------- Model Info End ----------------------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 忽略部分回显</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen] prompt = 你好, gen =</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 你好，我是资深技术支持专家，很高兴为您提供技术支持。请问有什么软硬件产品需要我帮助您解答疑问吗？</span><br></span></code></pre></div></div>
</li>
</ol>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mltc编译大模型">MLTC编译大模型<a href="#mltc编译大模型" class="hash-link" aria-label="Direct link to MLTC编译大模型" title="Direct link to MLTC编译大模型" translate="no">​</a></h3>
<ol>
<li>
<p>设置环境变量。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ export RISCV=/usr/local/hpe</span><br></span></code></pre></div></div>
</li>
<li>
<p>从HuggingFace等渠道获取<a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct" target="_blank" rel="noopener noreferrer">Qwen2-7B-Instruct</a>的文件，包括包括模型文件、权重文件、分词器文件、词表等，并复制到目标服务器。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-18-7903d9c94553bbd81d24818d822c65fa.png" width="1280" height="694" class="img_ev3q"></p>
</li>
<li>
<p>编写Python脚本部署并验证模型。脚本示例中主要包括以下步骤：</p>
<ol>
<li>
<p>准备模型配置，支持的配置项请参见<em>模型配置</em>章节。</p>
</li>
<li>
<p>调用<code>AsyncGeneration</code>接口部署模型。</p>
</li>
<li>
<p>发送messages验证LLM的对话效果。</p>
</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ cat test_qwen2-7b-instruct.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import asyncio</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from stc_llm_mltc.runtime import AsyncGeneration</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">async def test_run():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    config = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;model_name&quot;: &quot;Qwen/Qwen2-7B-Instruct&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;devices&quot;: [0, 1],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;tok_dir&quot;: &quot;/model/qwen2-7b-instruct/&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;mltc_weight_dir&quot;: &quot;/model/qwen2-7b-instruct-weight/qwen2-7b-32heads-mask-28layers&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    prompt = &quot;你是一个写作小助手，请帮忙写一篇描述江南春天长度不少于1024个token的小作文，要求其中必须涉及到描述朦胧的烟雨，蜿蜒的石板小路等景物，同时既要描写出阳光明媚、风和日丽的场景，也要描述出烟雨绵绵、云气氤氲的场景。然后文章中需要包含排比、比喻、对偶、拟人等修辞手法，必要时可以引用一些符合文章主题语境的古诗词中的句子以增加文采。&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    generation = AsyncGeneration(config)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    task = await generation.create_task(prompt=prompt, max_output_len=5)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    async for _ in generation.generate_stream(task):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        toks = task.gen_tokens[task.return_offset :]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        task.return_offset += len(toks)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if len(toks) == 0:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            continue</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        text = generation.tokenizer.decode(toks)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        print(text, end=&quot; &quot;, flush=True)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">asyncio.run(test_run())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ python3 test_qwen2-7b-instruct.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 忽略部分回显</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">江南的春天，如同一位羞涩的少女，轻启朱唇，吐露着温柔的气息。她以一种难以言喻的美，缓缓铺展开一幅幅细腻的画卷，让人心醉神迷。在这片土地上，春天的长度仿佛被时间的笔触拉长，每一刻都充满了诗意与生机。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">烟雨是江南春天的魂魄，它轻柔地拂过每一寸土地，为大地披上了一层薄薄的轻纱。那烟雨，如同细丝般缠绕在古老的石板小路上，蜿蜒曲折，仿佛是大自然的笔触，勾勒出一幅幅水墨画。雨滴落在青石板上，发出清脆的响声，如同古琴的低吟，悠扬而深邃。这烟雨，是江南春天的序曲，它以一种朦胧而神秘的方式，唤醒了沉睡的大地，让万物在湿润中苏醒。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">阳光明媚的日子，江南的春天则展现出另一番景象。阳光如同金色的丝线，穿透云层，洒在大地上，给万物披上了一层金色的外衣。微风轻拂，带着花香与泥土的芬芳，让人感到心旷神怡。在这温暖的阳光下，小溪潺潺，柳树轻摇，仿佛在诉说着春天的故事。这阳光，是江南春天的主旋律，它以一种明亮而温暖的方式，照亮了大地，让世界充满了生机与活力。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">在这片土地上，春天的美是多维度的，既有烟雨的朦胧，也有阳光的明媚。它们交织在一起，如同一幅动人的画卷，让人沉醉其中。正如唐代诗人杜甫所言：“好雨知时节，当春乃发生。随风潜入夜，润物细无声。”这正是江南春天的写照，它在不经意间，以一种润物细无声的方式，将美与希望播撒在大地上。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">江南的春天，是一首诗，是一幅画，更是一段旅程。在这段旅程中，每一处景致都充满了诗意与哲思，让人在欣赏之余，也能感受到生命的美好与希望。春天的江南，以其独特的魅力，吸引着无数人前来探寻，感受那份属于春天的温柔与力量。 &lt;|im_end|&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="模型配置">模型配置<a href="#模型配置" class="hash-link" aria-label="Direct link to 模型配置" title="Direct link to 模型配置" translate="no">​</a></h3>
<table><thead><tr><th><strong>配置项</strong></th><th><strong>是否必填</strong></th><th><strong>数据类型</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>model_name</td><td>是</td><td>str</td><td>待部署模型的名称，建议与HuggingFace等渠道中的名称保持一致。</td></tr><tr><td>devices</td><td>是</td><td>list</td><td>模型推理时占用NPU设备的ID列表，STC_LLM会根据NPU设备的数量将原始权重自动切分为匹配的份数。</td></tr><tr><td>tok_dir</td><td>是</td><td>str</td><td>加载模型的分词器（tokenizer）文件的路径。</td></tr><tr><td>original_weight_dir</td><td>否</td><td>str</td><td>加载模型的原始权重文件的路径。如果您没有在目标服务器上转换过权重，则至少需要从原始权重转换一次，这时<code>original_weight_dir</code>为必填项。</td></tr><tr><td>weight_dir</td><td>否</td><td>str</td><td>从原始权重转换后可以在NPU设备上使用的权重的路径。<br>- 如果<code>weight_dir</code>指向的路径存在权重文件，则使用这些权重。<br>- 如果<code>weight_dir</code>指向的路径不存在权重文件，则STC_LLM尝试从<code>original_weight_dir</code>获取原始权重进行转换，并将转换后的权重放到<code>weight_dir</code>指向的路径。</td></tr><tr><td>mltc_weight_dir</td><td>是<br>说明：当采用MLTC编译方式时为必填。</td><td>str</td><td>采用MLTC编译方式的权重路径。</td></tr><tr><td>mltc_ht_file</td><td>否</td><td>str</td><td>采用MLTC编译方式时，vmfb文件存放的路径。</td></tr><tr><td>compile_args</td><td>否</td><td>str</td><td>采用MLTC编译方式时传入的编译参数，具体编译参数可参见<em>Python</em> <em>API</em>中的MLTC API。</td></tr><tr><td>max_tasks</td><td>否</td><td>int</td><td>支持同时执行的最大task数量，每个task分配一个stream完成一轮对话。默认值为256。</td></tr><tr><td>custom_parameters</td><td>否</td><td>bool</td><td>- True：允许task自定义temperature、top_p、top_k、logprobs、top_logprobs。<br>- False：默认值。禁止task自定义temperature、top_p、top_k、logprobs、top_logprobs。</td></tr><tr><td>temperature</td><td>否</td><td>float</td><td>超参数之一，控制生成输出的随机性，值越低保证更多的确定性，值越高引入更多的随机性。需要将custom_parameters配置为True才可以修改，默认值为0.5，数值范围为[0.0, 2.0]。</td></tr><tr><td>top_p</td><td>否</td><td>float</td><td>超参数之一，在生成输出抽样时排除累积概率低于该值的token。需要将custom_parameters配置为True才可以修改，默认值为0.2，数值范围为(0.0, 1.0]。</td></tr><tr><td>top_k</td><td>否</td><td>int</td><td>超参数之一，在生成输出抽样时只在概率top k个token中进行。需要将custom_parameters配置为True才可以修改，默认值为20，数值范围为[1, 词表长度]。</td></tr><tr><td>quant_type</td><td>否</td><td>str</td><td>指定是否通过量化节省内存空间。<br>- fp16：默认值，不启用压缩。<br>- w8a8：启用量化。<br>- w8a16：启用压缩。</td></tr><tr><td>compress_factor</td><td>否</td><td>int</td><td>压缩倍数，仅在<code>quant_type</code>为<code>w8a16</code>时生效，支持的压缩倍数包括128、64、32、16、8、1、-1，压缩倍数越高，能节省的内存空间越多。压缩倍数为-1时对应per-channel方式。</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="问题排查">问题排查<a href="#问题排查" class="hash-link" aria-label="Direct link to 问题排查" title="Direct link to 问题排查" translate="no">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="未设置risc-v环境变量">未设置RISC-V环境变量<a href="#未设置risc-v环境变量" class="hash-link" aria-label="Direct link to 未设置RISC-V环境变量" title="Direct link to 未设置RISC-V环境变量" translate="no">​</a></h4>
<ul>
<li>
<p>问题现象：执行脚本示例时报错<code>RuntimeError(&quot;Can&#x27;t find RISCV environment variable&quot;)</code>。</p>
</li>
<li>
<p>解决方式：必须设置RISC-V环境变量。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ export RISCV=/usr/local/hpe</span><br></span></code></pre></div></div>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="可用内存不足">可用内存不足<a href="#可用内存不足" class="hash-link" aria-label="Direct link to 可用内存不足" title="Direct link to 可用内存不足" translate="no">​</a></h4>
<ul>
<li>
<p>问题现象：执行脚本示例时报错<code>run weight convert ... Killed</code>。</p>
</li>
<li>
<p>解决方式：查看可用内存是否足以放下模型、权重等文件。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ free -h</span><br></span></code></pre></div></div>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="投机采样示例">投机采样示例<a href="#投机采样示例" class="hash-link" aria-label="Direct link to 投机采样示例" title="Direct link to 投机采样示例" translate="no">​</a></h2>
<ol>
<li>
<p>从HuggingFace等渠道获取<a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct" target="_blank" rel="noopener noreferrer">Qwen2-7B-Instruct</a>和Qwen2-0.5B-Instruct文件，包括模型文件、权重文件、分词器文件、词表等，并复制到目标服务器。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/stc-llm-image-19-7903d9c94553bbd81d24818d822c65fa.png" width="1280" height="694" class="img_ev3q"></p>
</li>
<li>
<p>编写Python脚本部署并验证模型。脚本示例中主要包括以下步骤：</p>
<ol>
<li>
<p>分别准备大小模型配置，支持的配置项请参见<em>模型配置</em>章节。</p>
</li>
<li>
<p>调用<code>ManualAsyncGeneration</code>接口部署模型。</p>
</li>
<li>
<p>发送messages验证LLM的对话效果。</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ cat test_speculative_decoding.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import asyncio</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from stc_llm_dnn.runtime import ManualAsyncGeneration</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">async def print_stream(gen, tokenizer, gen_name, prompt):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    async for aa in gen:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        print(&quot;[{}] prompt = {}, gen = {}&quot;.format(gen_name, prompt, tokenizer.decode(aa)))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">async def test_async_generation():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    # 配置信息</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    config = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;model_name&quot;: &quot;Qwen/Qwen2-7B-Instruct&quot;, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;tok_dir&quot;: &quot;./qwen2-7b-instruct/Qwen2-7B-Instruct/&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;original_weight_dir&quot;: &quot;./qwen2-7b-instruct/Qwen2-7B-Instruct&quot;, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;weight_dir&quot;: &quot;./qwen2-7b-instruct/Qwen2-7B-Instruct/2npu&quot;, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;devices&quot;: [0,1],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;engine_dir&quot;: &quot;./tmp_engine1&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;embed_on_host&quot;: False,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;output_on_host&quot;: False,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;slot_number_per_segment&quot;: 256,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;max_output_len&quot;: 25600,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;max_tasks&quot;: 256,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;custom_parameters&quot;: True,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    async_generation = ManualAsyncGeneration(config)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    assistant_config = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;model_name&quot;: &quot;Qwen/Qwen2-0.5B-Instruct&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;tok_dir&quot;: &quot;./qwen2-0.5b-instruct-weight/Qwen2-0.5B-Instruct/&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;original_weight_dir&quot;: &quot;./qwen2-0.5b-instruct/Qwen2-0.5B-Instruct&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;weight_dir&quot;: &quot;./qwen2-0.5b-instruct/Qwen2-0.5B-Instruct/1npu&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;devices&quot;: [2],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;engine_dir&quot;: &quot;./tmp_engine2&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;embed_on_host&quot;: False,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;output_on_host&quot;: False,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;slot_number_per_segment&quot;: 256,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;max_output_len&quot;: 25600,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;max_tasks&quot;: 256,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;custom_parameters&quot;: True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    assistant_generation = ManualAsyncGeneration(assistant_config)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    messages_easy_test = [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;介绍一下自己&quot;},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    user_tasks = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    for i in range(1):  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        task_id, gen = await async_generation.generate(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            messages = messages_easy_test,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            temperature=0.5,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            top_p=0.2,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            top_k=1,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            max_output_len=1024,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            assistant_generation=assistant_generation,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            assistant_len = 3,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        user_task = asyncio.create_task(print_stream(gen, async_generation.tokenizer, f&quot;gen{i}&quot;, messages_easy_test))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        user_tasks.append(user_task)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    for user_task in user_tasks:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        await user_task</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    await async_generation.shutdown()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">if __name__ == &quot;__main__&quot;:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    asyncio.run(test_async_generation()) </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ python3 test_chatglm3-6b.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">---------------------------- Model Info Begin ----------------------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model name: Qwen/Qwen2-7B-Instruct</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">npus: [0, 1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">total slot number: 122082</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">slot number per segment: 256</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">total segment number: 476</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bytes per slot: 14336</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">temporary cpp dir: ./tmp_engine1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">weight dir: ./qwen2-7b-instruct/Qwen2-7B-Instruct/2npu</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">embedding on host: False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">output on host: False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dma preload: True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">max sequence length: 131072</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">compress_factor: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">quant_type: fp16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">---------------------------- Model Info End ----------------------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">---------------------------- Model Info Begin ----------------------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model name: Qwen/Qwen2-0.5B-Instruct</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">npus: [2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">total slot number: 488260</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">slot number per segment: 256</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">total segment number: 1907</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bytes per slot: 6144</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">temporary cpp dir: ./tmp_engine2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">weight dir: ./qwen2-0.5b-instruct/Qwen2-0.5B-Instruct/1npu</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">embedding on host: False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">output on host: False</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dma preload: True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">max sequence length: 32768</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">compress_factor: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">quant_type: fp16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">---------------------------- Model Info End ----------------------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen =</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 你好！</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 我是一个</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 大模型，</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 全名叫</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 通义千问</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 作为一个AI助手，</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 我的</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 主要</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 功能是</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 帮助用户</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 解答问题、提供</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 信息、</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 进行</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 对话</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 等。我被</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 训练</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 了</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 在</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 各种</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 主题上提供</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 知识和</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 见解，包括但不限于</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 科技、</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 文化、历史、</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 教育、生活</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 等领域。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 无论</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 你需要</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 学习新知识、</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 完成</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 某个任务，还是</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 只是想</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 聊天</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 解闷，我</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 都会</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 尽力提供帮助。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 请随时</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 向我提问，</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 我会尽力</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 给出</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 准确</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 、有用</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 的回答。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 如果你有任何</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 需要</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 或者</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 好奇的问题，欢迎</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 随时</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen = 告诉我！</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[gen0] prompt = [{&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;介绍一下自己&#x27;}], gen =</span><br></span></code></pre></div></div>
</li>
</ol>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="问题排查-1">问题排查<a href="#问题排查-1" class="hash-link" aria-label="Direct link to 问题排查" title="Direct link to 问题排查" translate="no">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="可用内存不足-1">可用内存不足<a href="#可用内存不足-1" class="hash-link" aria-label="Direct link to 可用内存不足" title="Direct link to 可用内存不足" translate="no">​</a></h4>
<ul>
<li>
<p>问题现象：执行脚本示例时报错<code>Bus error</code>。</p>
</li>
<li>
<p>解决方式：使用<code>free -h</code>查看可用内存是否足以放下模型、权重等文件。在转换权重时还会创建副本，因此需要配备尽量大的内存，也可直接使用<code>weight_dir</code>设置转换后的权重。</p>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/推理软件使用手册/STCRP使用指南/STC_LLM使用指南.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/推理软件使用手册/STCRP使用指南/MLTC使用指南"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">MLTC使用指南</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/category/算力服务解决方案"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">算力服务解决方案</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#stc_llm概述" class="table-of-contents__link toc-highlight">STC_LLM概述</a></li><li><a href="#前提条件" class="table-of-contents__link toc-highlight">前提条件</a></li><li><a href="#提供推理服务" class="table-of-contents__link toc-highlight">提供推理服务</a><ul><li><a href="#服务端启动推理服务" class="table-of-contents__link toc-highlight">服务端启动推理服务</a></li><li><a href="#客户端发起推理请求" class="table-of-contents__link toc-highlight">客户端发起推理请求</a></li></ul></li><li><a href="#监控推理指标" class="table-of-contents__link toc-highlight">监控推理指标</a><ul><li><a href="#前提条件-1" class="table-of-contents__link toc-highlight">前提条件</a></li><li><a href="#指标类型" class="table-of-contents__link toc-highlight">指标类型</a></li><li><a href="#服务端操作" class="table-of-contents__link toc-highlight">服务端操作</a></li><li><a href="#web端操作prometheus" class="table-of-contents__link toc-highlight">Web端操作（Prometheus）</a></li><li><a href="#web端操作grafana" class="table-of-contents__link toc-highlight">Web端操作（Grafana）</a></li></ul></li><li><a href="#生成量化系数" class="table-of-contents__link toc-highlight">生成量化系数</a></li><li><a href="#本地验证模型" class="table-of-contents__link toc-highlight">本地验证模型</a><ul><li><a href="#手写大模型" class="table-of-contents__link toc-highlight">手写大模型</a></li><li><a href="#mltc编译大模型" class="table-of-contents__link toc-highlight">MLTC编译大模型</a></li><li><a href="#模型配置" class="table-of-contents__link toc-highlight">模型配置</a></li><li><a href="#问题排查" class="table-of-contents__link toc-highlight">问题排查</a></li></ul></li><li><a href="#投机采样示例" class="table-of-contents__link toc-highlight">投机采样示例</a><ul><li><a href="#问题排查-1" class="table-of-contents__link toc-highlight">问题排查</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">希姆计算</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.streamcomputing.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">希姆计算官网<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.streamcomputing.com/index.php?s=about&amp;c=category&amp;id=1#a1" target="_blank" rel="noopener noreferrer" class="footer__link-item">关于希姆计算<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.streamcomputing.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">联系我们<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">开源</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/riscv-stc/riscv-matrix-project" target="_blank" rel="noopener noreferrer" class="footer__link-item">自研AI计算矩阵扩展指令集<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/Stream-Computing/STCPaddleModelZoo" target="_blank" rel="noopener noreferrer" class="footer__link-item">百度飞桨 x 希姆计算AI模型库<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">资源</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://riscv.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">RISC-V International<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a class="footer__link-item" href="/docs/希姆计算术语表">希姆计算术语表</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 广州希姆半导体科技有限公司Stream Computing Inc.</div></div></div></footer></div>
</body>
</html>